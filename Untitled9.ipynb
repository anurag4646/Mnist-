{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPenjDv0DYsGDYwyu+CSeay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anurag4646/Mnist-/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6HSydJx2QNA"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline\n",
        "\n",
        "# Use a white background for matplotlib figures\n",
        "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXMVjtqr2V9b"
      },
      "source": [
        "dataset = MNIST(root='data/', download=True, transform=ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77gD6VD02b-w"
      },
      "source": [
        "### Let's look at a couple of images from the dataset. The images are converted to PyTorch tensors with the shape 1x28x28 \n",
        "### (the dimensions represent color channels, width and height). We can use plt.imshow to display the images. However, \n",
        "### plt.imshow expects channels to be last dimension in an image tensor, so we use the permute method to reorder the dimensions of the image."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4PzDmrb2qBI"
      },
      "source": [
        "image, label = dataset[0]\n",
        "print('image.shape:', image.shape)\n",
        "prmute_img = image.permute(1, 2, 0)\n",
        "plt.imshow(image[0], cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZuzB8b-2xtI"
      },
      "source": [
        "image, label = dataset[1]\n",
        "print('image.shape:', image.shape)\n",
        "plt.imshow(image[0], cmap='gray')\n",
        "print('Label:', label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF6Pj0_X21Sg"
      },
      "source": [
        "## Next, let's use the random_split helper function to set aside 10000 images for our validation set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlYlQCnT24-M"
      },
      "source": [
        "val_size = 10000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgZ9jyPQ27bD"
      },
      "source": [
        "## We can now create PyTorch data loaders for training and validation."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlyEIDlH3HLw"
      },
      "source": [
        "batch_size=128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Cyq8i13BVa"
      },
      "source": [
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyzf1g5G3Qun"
      },
      "source": [
        "Let's visualize a batch of data in a grid using the make_grid function from torchvision. We'll also use the .permute method on the tensor to move the channels to the last dimension, as expected by matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQUrmsY13D9_"
      },
      "source": [
        "for images, _ in train_loader:\n",
        "    print('images.shape:', images.shape)\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YblOk2C3fRV"
      },
      "source": [
        "Hidden Layers, Activation Functions and Non-Linearity\n",
        "We'll create a neural network with two layers: a hidden layer and an output layer. Additionally, we'll use an activation function between the two layers. Let's look at a step-by-step example to learn how hidden layers and activation functions can help capture non-linear relationships between inputs and outputs.\n",
        "\n",
        "First, let's create a batch of inputs tensors. We'll flatten the 1x28x28 images into vectors of size 784, so they can be passed into an nn.Linear object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_m4I2fB3UEb"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print('images.shape:', images.shape)\n",
        "    inputs = images.reshape(-1, 784)\n",
        "    print('inputs.shape:', inputs.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbgroZk03oGv"
      },
      "source": [
        "Next, let's create a nn.Linear object, which will serve as our hidden layer. We'll set the size of the output from the hidden layer to 32. This number can be increased or decreased to change the learning capacity of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tESHs5k3jfz"
      },
      "source": [
        "input_size = inputs.shape[-1]\n",
        "hidden_size = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxsX-yLq3rrj"
      },
      "source": [
        "input_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6QOrEUe3uJu"
      },
      "source": [
        "layer1 = nn.Linear(input_size, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHGbuZGE3xnX"
      },
      "source": [
        "## We can now compute intermediate outputs for the batch of images by passing inputs through layer1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHhG-NEW30t7"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHrEA69t33Ke"
      },
      "source": [
        "layer1_outputs = layer1(inputs)\n",
        "print('layer1_outputs.shape:', layer1_outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SXLZMQu4ERJ"
      },
      "source": [
        "The image vectors of size 784 are transformed into intermediate output vectors of length 32 by performing a matrix multiplication of inputs matrix with the transposed weights matrix of layer1 and adding the bias. We can verify this using torch.allclose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZZrDat64DeD"
      },
      "source": [
        "layer1_outputs_direct = inputs @ layer1.weight.t() + layer1.bias\n",
        "layer1_outputs_direct.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvydGsqm36pv"
      },
      "source": [
        "torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhdcVHhZ4YDJ"
      },
      "source": [
        "Thus, layer1_outputs and inputs have a linear relationship, i.e., each element of layer_outputs is a weighted sum of elements from inputs. Thus, even as we train the model and modify the weights, layer1 can only capture linear relationships between inputs and outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEpqtJ414aWu"
      },
      "source": [
        "Next, we'll use the Rectified Linear Unit (ReLU) function as the activation function for the outputs. It has the formula relu(x) = max(0,x) i.e. it simply replaces negative values in a given tensor with the va.lue 0. ReLU is a non-linear function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKrMgM5j4jfb"
      },
      "source": [
        "Let's apply the activation function to layer1_outputs and verify that negative values were replaced with 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIN9ufUw4KTR"
      },
      "source": [
        "relu_outputs = F.relu(layer1_outputs)\n",
        "print('min(layer1_outputs):', torch.min(layer1_outputs).item())\n",
        "print('min(relu_outputs):', torch.min(relu_outputs).item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur9fIG9I4ql_"
      },
      "source": [
        "Now that we've applied a non-linear activation function, relu_outputs and inputs do not have a linear relationship. We refer to ReLU as the activation function, because for each input certain outputs are activated (those with non-zero values) while others turned off (those with zero values)\n",
        "\n",
        "Next, let's create an output layer to convert vectors of length hidden_size in relu_outputs into vectors of length 10, which is the desired output of our model (since there are 10 target labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMSNe8Kp4nQr"
      },
      "source": [
        "output_size = 10\n",
        "layer2 = nn.Linear(hidden_size, output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q9Sr-A94ukS"
      },
      "source": [
        "layer2_outputs = layer2(relu_outputs)\n",
        "print(layer2_outputs.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzw7Bpnl4xXm"
      },
      "source": [
        "inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjyczXFa4239"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AikFPlJppeT7"
      },
      "source": [
        "As expected, `layer2_outputs` contains a batch of vectors of size 10. We can now use this output to compute the loss using `F.cross_entropy` and adjust the weights of `layer1` and `layer2` using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UWEYPMp4zm_"
      },
      "source": [
        "F.cross_entropy(layer2_outputs, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng8V_TKO4-SF"
      },
      "source": [
        "Thus, our model transforms inputs into layer2_outputs by applying a linear transformation (using layer1), followed by a non-linear activation (using F.relu), followed by another linear transformation (using layer2). Let's verify this by re-computing the output using basic matrix operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aSRHHCU460A"
      },
      "source": [
        "# Expanded version of layer2(F.relu(layer1(inputs)))\n",
        "outputs = (F.relu(inputs @ layer1.weight.t() + layer1.bias)) @ layer2.weight.t() + layer2.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ7xDs2D5DWO"
      },
      "source": [
        "torch.allclose(outputs, layer2_outputs, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_0sPJH5F1_"
      },
      "source": [
        "# Same as layer2(layer1(inputs))\n",
        "outputs2 = (inputs @ layer1.weight.t() + layer1.bias) @ layer2.weight.t() + layer2.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5HFpHza5RWk"
      },
      "source": [
        "# Create a single layer to replace the two linear layers\n",
        "combined_layer = nn.Linear(input_size, output_size)\n",
        "\n",
        "combined_layer.weight.data = layer2.weight @ layer1.weight\n",
        "combined_layer.bias.data = layer1.bias @ layer2.weight.t() + layer2.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQQvAOBf5UR9"
      },
      "source": [
        "# Same as combined_layer(inputs)\n",
        "outputs3 = inputs @ combined_layer.weight.t() + combined_layer.bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwoGj9mw5Xhu"
      },
      "source": [
        "torch.allclose(outputs2, outputs3, 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl1SkH_q5ic9"
      },
      "source": [
        "Model : - \n",
        "We are now ready to define our model. As discussed above, we'll create a neural network with one hidden layer. Here's what that means:\n",
        "\n",
        "Instead of using a single nn.Linear object to transform a batch of inputs (pixel intensities) into outputs (class probabilities), we'll use two nn.Linear objects. Each of these is called a layer in the network.\n",
        "\n",
        "The first layer (also known as the hidden layer) will transform the input matrix of shape batch_size x 784 into an intermediate output matrix of shape batch_size x hidden_size. The parameter hidden_size can be configured manually (e.g., 32 or 64).\n",
        "\n",
        "We'll then apply a non-linear activation function to the intermediate outputs. The activation function transforms individual elements of the matrix.\n",
        "\n",
        "The result of the activation function, which is also of size batch_size x hidden_size, is passed into the second layer (also known as the output layer). The second layer transforms it into a matrix of size batch_size x 10. We can use this output to compute the loss and adjust weights using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97NEST8l5aJu"
      },
      "source": [
        "class MnistModel(nn.Module):\n",
        "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
        "    def __init__(self, in_size, hidden_size, out_size):\n",
        "        super().__init__()\n",
        "        # hidden layer\n",
        "        self.linear1 = nn.Linear(in_size, hidden_size)\n",
        "        # output layer\n",
        "        self.linear2 = nn.Linear(hidden_size, out_size)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten the image tensors\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        # Get intermediate outputs using hidden layer\n",
        "        out = self.linear1(xb)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht-8tV9f59NL"
      },
      "source": [
        "We also need to define an accuracy function which calculates the accuracy of the model's prediction on an batch of inputs. It's used in validation_step above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1luOaPxh56N2"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbcnY-Uu6FGD"
      },
      "source": [
        "We'll create a model that contains a hidden layer with 32 activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS9ik_nI6BOx"
      },
      "source": [
        "input_size = 784\n",
        "hidden_size = 32 # you can change this\n",
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IoPtWHB6Ol1"
      },
      "source": [
        "model = MnistModel(input_size, hidden_size=32, out_size=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_HAQI396OYV"
      },
      "source": [
        "for t in model.parameters():\n",
        "    print(t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Obd3ut6IGY"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    outputs = model(images)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    print('Loss:', loss.item())\n",
        "    break\n",
        "\n",
        "print('outputs.shape : ', outputs.shape)\n",
        "print('Sample outputs :\\n', outputs[:2].data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRRFLbq36Xru"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKBb0ADI6cR4"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zxLS4_Z6cMg"
      },
      "source": [
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU7aozGq62S-"
      },
      "source": [
        "Next, let's define a function that can move data and model to a chosen device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWgSyin76cJw"
      },
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRchIiy966Ni"
      },
      "source": [
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    images = to_device(images, device)\n",
        "    print(images.device)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QJ6MLh7LqY"
      },
      "source": [
        "Finally, we define a DeviceDataLoader class to wrap our existing data loaders and move batches of data to the selected device. Interestingly, we don't need to extend an existing class to create a PyTorch datal oader. All we need is an __iter__ method to retrieve batches of data and an __len__ method to get the number of batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC033EH469JP"
      },
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxwvU5eP4r76"
      },
      "source": [
        "The `yield` keyword in Python is used to create a generator function that can be used within a `for` loop, as illustrated below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cu0b6pS7ea8"
      },
      "source": [
        "We can now wrap our data loaders using DeviceDataLoader.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oUDNj1V7QEZ"
      },
      "source": [
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbTY1ZJv7ny1"
      },
      "source": [
        "Tensors moved to the GPU have a device property which includes that word cuda. Let's verify this by looking at a batch of data from valid_dl."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5JJAa3_7kVv"
      },
      "source": [
        "for xb, yb in val_loader:\n",
        "    print('xb.device:', xb.device)\n",
        "    print('yb:', yb)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPG6PUse7xvi"
      },
      "source": [
        "Training the Model\n",
        "We'll define two functions: fit and evaluate to train the model using gradient descent and evaluate its performance on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylyGKx5s7rSF"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    \"\"\"Evaluate the model's performance on the validation set\"\"\"\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
        "    \"\"\"Train the model using gradient descent\"\"\"\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gn8cT7n781m"
      },
      "source": [
        "Before we train the model, we need to ensure that the data and the model's parameters (weights and biases) are on the same device (CPU or GPU). We can reuse the to_device function to move the model's parameters to the right device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPERHxb74Lc"
      },
      "source": [
        "# Model (on GPU)\n",
        "model = MnistModel(input_size, hidden_size=hidden_size, out_size=num_classes)\n",
        "to_device(model, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK6wnmoA8ER2"
      },
      "source": [
        "Let's see how the model performs on the validation set with the initial set of weights and biases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRVLI2az8BFE"
      },
      "source": [
        "history = [evaluate(model, val_loader)]\n",
        "history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j-OyzXJ8L6b"
      },
      "source": [
        "The initial accuracy is around 10%, as one might expect from a randomly initialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n",
        "\n",
        "Let's train the model for five epochs and look at the results. We can use a relatively high learning rate of 0.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqrkT9az8HY0"
      },
      "source": [
        "history += fit(5, 0.5, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY-v_yOP8PK3"
      },
      "source": [
        "history += fit(5, 0.1, model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48TnPSh8aPC"
      },
      "source": [
        "We can now plot the losses & accuracies to study how the model improves over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KJF8u5r8USQ"
      },
      "source": [
        "losses = [x['val_loss'] for x in history]\n",
        "plt.plot(losses, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Loss vs. No. of epochs');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqAzw6nk8eVq"
      },
      "source": [
        "accuracies = [x['val_acc'] for x in history]\n",
        "plt.plot(accuracies, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs. No. of epochs');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5y2OeJ18voj"
      },
      "source": [
        "It quickly reaches an accuracy of 97% but doesn't improve much beyond this. To improve accuracy further, we need to make the model more powerful by increasing the hidden layer's size or adding more hidden layers with activations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UirisGL84rP"
      },
      "source": [
        "As a final step, let's also look at the overall loss and accuracy of the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWVnx6Mx9KrF"
      },
      "source": [
        "# Define test dataset\n",
        "test_dataset = MNIST(root='data/', \n",
        "                     train=False,\n",
        "                     transform=ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4vlZH6N8ikZ"
      },
      "source": [
        "test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size=256), device)\n",
        "result = evaluate(model, test_loader)\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR76ry7f9l6D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}